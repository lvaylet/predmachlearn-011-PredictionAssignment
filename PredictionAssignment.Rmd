---
title: "PredictionAssignment"
author: "Laurent VAYLET"
date: "Feb 13, 2015"
output: html_document
---

# Background

Using devices such as *Jawbone Up*, *Nike FuelBand* and *Fitbit*, it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify *how well they do it*. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

# Objective

The goal of this project is to predict the manner in which the 6 participants did the exercise. This corresponds to the "classe" variable in the training set. Any of the other variables can be used to predict with.

This report describes how the model is built, how cross validation was used, what the expected out of sample error is, and why the choices were made. The prediction model will also be used to predict 20 different test cases.

# Approach

1. Download and read training and test data
2. Remove unnecessary covariates (lot of NA values, user names...)
3. Split training set for cross-validation
4. Use PCA to reduce even further the number of covariates
5. Train a Random Forest model.
6. Assess model performance (confusion matrix)
7. Use model on test data

Benefits of Random Forests are:

* Accuracy
* Runs efficiently on large data bases
* Handles thousands of input variables without variable deletion
* Gives estimates of what variables are important in the classification

# Libraries

First, load libraries used for data exploration and prediction:
```{r}
library(caret)
library(corrplot)
```

# Data

If necessary, download the data set for training and testing:
```{r}
if (!file.exists("./pml-training.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                destfile = "./pml-training.csv")
}
if (!file.exists("./pml-testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile = "./pml-testing.csv")
}
```

Read training and test data sets, replacing any empty value with NA (that will be useful later on when counting NA values):

```{r}
pml_training <- read.csv("pml-training.csv", na.strings=c("","NA"))
pml_testing <- read.csv("pml-testing.csv", na.string=c("","NA"))
```

# Exploration and pre-processing

Display structure of training data set in order to quickly get some insight in a compact form:

```{r}
str(pml_training)
```

At first sight, it looks like some covariates contain a lot of NA values. What is the exact proportion of NA values in every covariate?

```{r}
colSums(is.na(pml_training))/nrow(pml_training)
```

Wow, it looks lire more than half of the covariates have more than 97% NA values! Should we remove these covariates, or rather the examples they are associated with? Usually, it is better to keep a lot of observations rather than a lot of covariates, as having too many features can lead to overfitting while having too many examples is not that bad. So, in this case, it is better to remove the columns with a lot of NA, rather than removing the rows (using `complete.cases()`, for instance).

So keep only the columns with no NA values, and discard the others:

```{r}
pml_training <- pml_training[,colSums(is.na(pml_training)) == 0]
```

Now we are down to `r ncol(pml_training)` covariates (as opposed to 160 at the beginning).

Let's remove some more unnecessary columns (timestamps or factor variables that cannot be used for correlation computing later on):

```{r}
pml_training <- subset(pml_training, select=-c(X,
                                               user_name,
                                               raw_timestamp_part_1,
                                               raw_timestamp_part_2,
                                               cvtd_timestamp,
                                               new_window))
```

Now we are just left with `r ncol(pml_training)` compared to 160 for the raw unprocessed data.

Split the training data into two sets for cross-validation (with the usual 60-40 ratio for training and testing):

```{r}
inTrain <- createDataPartition(y=pml_training$class, p=0.6, list=FALSE)
training <- pml_training[inTrain,]
testing <- pml_training[-inTrain,]
dim(training)
dim(testing)
```

Being left with only `r ncol(pml_training)` covariates is good. However, we can try to reduce this number even further by identifying correlation between covariates and applying Principal Component Analysis to lower the number of features. Even if Random Forest can handle thousands of input variables without variable deletion, computation time is a precious resource.

So compute and plot correlation matrix to identify correlated features (excluding the "classe" factor we actually want to predict):

```{r, out.width = '\\maxwidth'}
corMat <- cor(subset(training, select=-c(classe)))
corrplot(corMat, order = "FPC", method = "color", type = "lower", tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

Find the covariates with a correlation factor greater than 80% (excluding the diagonal that is always equal to 100%)

```{r}
corMat[(corMat < -0.8 | corMat > 0.8) & corMat != 1]
```

Now train a Random Forest model using the full training set and a specified seed for reproducible :

```{r}
set.seed(1234)
t1 <- Sys.time()
modelFit <- train(training$classe ~ ., preProcess=c("center","scale"), method="rf", 
                  trControl=trainControl(method="cv"), data=training)
t2 <- Sys.time()
t2 - t1
modelFit
modelFit$finalModel
```

Apply model to test data and display confusion matrix:

```{r}
predictions <- predict(modelFit, testing)
confusionMatrix(predictions, testing$classe)
```

Now let's try PCA and see if it reduces computation time while keeping the same level of accuracy.

Perform PCA on all predictors (except the output "classe") and get the number of principal components necessary for retaining 95% of the variance:

```{r}
preProc <- preProcess(subset(training, select=-c(classe)), method=c("center","scale","pca"), thresh=0.95)
trainingPC <- predict(preProc, subset(training, select=-c(classe)))
```

Now train a Random Forest model using the reduced data from PCA and a specified seed for reproducible :

```{r}
set.seed(1234)
t1 <- Sys.time()
modelFit <- train(training$classe ~ ., method="rf", 
                  trControl=trainControl(method="cv"), data=trainingPC)
t2 <- Sys.time()
t2 - t1
modelFit
modelFit$finalModel
```

Apply model to test data and display confusion matrix:

```{r}
testingPC <- predict(preProc, subset(testing, select=-c(classe)))
predictions <- predict(modelFit, testingPC)
confusionMatrix(predictions, testing$classe)
```
