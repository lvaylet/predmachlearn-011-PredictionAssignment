---
title: "PredictionAssignment"
author: "Laurent VAYLET"
date: "Feb 13, 2015"
output: html_document
---

# Background

Using devices such as *Jawbone Up*, *Nike FuelBand* and *Fitbit*, it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify *how well they do it*. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

# Objective

The goal of this project is to predict the manner in which the 6 participants did the exercise. This corresponds to the "classe" variable in the training set. Any of the other variables can be used to predict with.

This report describes how the model is built, how cross validation was used, what the expected out of sample error is, and why the choices were made. The prediction model will also be used to predict 20 different test cases.

# Approach

1. Download and read training/test data
2. Clean data by removing unnecessary variables (columns with a lot of NA values, user names, timestamps...)
3. Slice training data for cross-validation
4. Use PCA to reduce the number of covariates
5. Train a Random Forest model on dimensionality-reduced training set
6. Assess model performance on validation set
7. Assess model performance on test set

# Data reading

If necessary, download the data set for training and testing:
```{r}
if (!file.exists("./pml-training.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                destfile = "./pml-training.csv")
}
if (!file.exists("./pml-testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile = "./pml-testing.csv")
}
```

Read training and test data sets, replacing any empty value with NA (that will be useful later on when counting NA values):

```{r}
training.raw <- read.csv("pml-training.csv", na.strings=c("","NA"))
testing.raw <- read.csv("pml-testing.csv", na.string=c("","NA"))
```

# Data cleaning

Display structure of training data set in order to quickly get some insight in a compact form:

```{r}
str(training.raw)
```

At first sight, it looks like some covariates contain a lot of NA values. What is the exact proportion of NA values in every covariate?

```{r}
colSums(is.na(training.raw))/nrow(training.raw)
```

Wow, it looks lire more than half of the covariates have more than 97% NA values! Should we remove these covariates, or rather the examples they are associated with? Usually, it is better to keep a lot of observations rather than a lot of covariates, as having too many features can lead to overfitting while having too many examples is not that bad. So, in this case, it is better to remove the columns with a lot of NA, rather than removing the rows (using `complete.cases()`, for instance).

So keep only the columns with no NA values, and discard the others:

```{r}
keep.cols = colSums(is.na(training.raw)) == 0
training.tidy <- training.raw[, keep.cols]
testing.tidy <- testing.raw[, keep.cols]
```

Now we are down to `r ncol(pml_training)` covariates (as opposed to 160 at the beginning).

Let's remove some more unnecessary columns (timestamps or factor variables that cannot be used for correlation computing later on):

```{r}
training.tidy <- subset(training.tidy, select = -c(X,
                                                   user_name,
                                                   raw_timestamp_part_1,
                                                   raw_timestamp_part_2,
                                                   cvtd_timestamp,
                                                   new_window,
                                                   num_window))
testing.tidy <- subset(testing.tidy, select = -c(X,
                                                 user_name,
                                                 raw_timestamp_part_1,
                                                 raw_timestamp_part_2,
                                                 cvtd_timestamp,
                                                 new_window,
                                                 num_window))
```

Now we are just left with `r ncol(pml_training)` compared to 160 for the raw unprocessed data.

# Data slicing

Split the training data into two sets for cross-validation (with the usual 60-40 ratio for training and testing):

```{r}
library(caret)
set.seed(12345)
inTrain <- createDataPartition(y = training.tidy$class, p = 0.6, list = FALSE)
train <- training.tidy[inTrain, ]
valid <- training.tidy[-inTrain, ]
test <- testing.tidy
dim(train)
dim(valid)
dim(test)
```

# Dimensionality reduction with PCA

Being left with only `r ncol(train)` covariates is good. However, we can try to reduce this number even further by identifying correlation between covariates and applying Principal Component Analysis to lower the number of features. Even if Random Forest can handle thousands of input variables without variable deletion, computation time is a precious resource.

First, compute and plot correlation matrix to identify potentially correlated features (excluding the "classe" factor we actually want to predict):

```{r, out.width = '\\maxwidth'}
library(corrplot)
corMat <- cor(subset(train, select=-c(classe)))
corrplot(corMat, method = "color", type = "lower", tl.cex = 0.8)
```

Find the covariates with a correlation factor greater than 80% (excluding the diagonal that is always equal to 100%)

```{r}
corMat[(corMat < -0.8 | corMat > 0.8) & corMat != 1]
```

Now perform PCA on all predictors (except the output "classe") and get the number of principal components necessary for retaining 95% of the variance:

```{r}
preProc.pca <- preProcess(subset(train, select = -c(classe)), method  = "pca", thresh = 0.95)
train.pca <- predict(preProc.pca, subset(train, select = -c(classe)))
valid.pca <- predict(preProc.pca, subset(valid, select = -c(classe)))
test.pca <- predict(preProc.pca, subset(test, select = -c(classe)))
print(preProc.pca)
```

# Model training and tuning

Now train a Random Forest model using the reduced data from PCA.

Random Forest was chosen as it offers the following benefits:

* Accuracy
* Runs efficiently on large data bases
* Handles thousands of input variables without variable deletion
* Gives estimates of what variables are important in the classification

```{r}
library(randomForest)
t1 <- Sys.time()
modelFit <- train(train$classe ~ ., method = "rf", trControl = trainControl(method = "cv"), data = train.pca)
t2 <- Sys.time()
t2 - t1
modelFit
modelFit$finalModel
```

Display dotchart of Principal Component importance as measured by Random Forest:

```{r}
varImpPlot(modelFit, sort = TRUE, main = "Relative importance of Principal Components")
```

# Model performance on validation dataset

Apply the model to the validation data and display the confusion matrix:

```{r}
valid.pred <- predict(modelFit, valid.pca)
confusionMatrix(valid.pred, valid$classe)
```

# Model performance on test dataset

Finally, apply the model to the 20 test cases:

```{r}
test.pred <- predict(modelFit, test.pca)
confusionMatrix(test.pred, test$classe)
```
